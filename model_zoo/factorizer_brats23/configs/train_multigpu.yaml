device: $torch.device('cuda:' + os.environ['LOCAL_RANK'])

network:
    _target_: torch.nn.parallel.DistributedDataParallel
    module: $@network.yaml#network_def.to(@device)
    device_ids: ["@device"]

train_sampler:
    _target_: DistributedSampler
    dataset: "@train_dataset"
    even_divisible: true
    shuffle: true
train_dataloader#sampler: "@train_sampler"
train_dataloader#shuffle: false
trainer#train_handlers: "$@train_handlers[: -2 if dist.get_rank() > 0 else None]"

val_ampler:
    _target_: DistributedSampler
    dataset: "@val_dataset"
    even_divisible: false
    shuffle: false
val_dataloader#sampler: "@val_sampler"
evaluator#val_handlers: $None if dist.get_rank() > 0 else @val_handlers

initialize:
    - $import torch.distributed as dist
    - $dist.is_initialized() or dist.init_process_group(backend='nccl')
    - $torch.cuda.set_device(@device)
    - $monai.utils.set_determinism(seed=123)
    - $setattr(torch.backends.cudnn, 'benchmark', True)
    - $@ckpt_loader.attach(@trainer) if os.path.exists(@ckpt_path) else None
    - $@ckpt_saver.attach(@trainer)

run:
    - $@trainer.run()

finalize:
    - $dist.is_initialized() and dist.destroy_process_group()
